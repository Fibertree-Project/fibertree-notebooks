{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Swap ranks using update batching\n",
    "\n",
    "This notebook explores the concept of swapping a tensor's ranks using update batching.\n",
    "\n",
    "See [update batching example](../graphs/update-batching.ipynb) for a simple example of update batching for a graph algorithm.\n",
    "\n",
    "First, include some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin - startup boilerplate code\n",
    "\n",
    "import pkgutil\n",
    "\n",
    "if 'fibertree_bootstrap' not in [pkg.name for pkg in pkgutil.iter_modules()]:\n",
    "  !python3 -m pip  install git+https://github.com/Fibertree-project/fibertree-bootstrap --quiet\n",
    "\n",
    "# End - startup boilerplate code\n",
    "\n",
    "\n",
    "from fibertree_bootstrap import *\n",
    "fibertree_bootstrap(style=\"tree\", animation=\"movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a rank-2 tensor\n",
    "\n",
    "Create a random rank-2 tensor (__a__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = 10\n",
    "R0 = R1\n",
    "\n",
    "a = Tensor.fromRandom([\"R1\", \"R0\"], [R1, R0], (1.0, 0.28), 9, seed=100)\n",
    "a.setColor(\"green\")\n",
    "a.setName(\"A\")\n",
    "\n",
    "displayTensor(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-pass transpose\n",
    "\n",
    "Process a transpose in one pass by doing a concordant traversal of __a__ and using __getPayloadRef()__ to insert/update a fiber in the top rank of __a_swapped__ at coordinate __r0__ for each element of __a__ and append the value (from __a__) into that fiber with coordinate __r1__.\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Traversal of __a__ is concordant.\n",
    "\n",
    "- Update traveral of top rank (__R0__) of __a_swapped__ is discordant.\n",
    "\n",
    "- Traveral of bottom rank (__R1__) of __a_swapped__ is always an append, but to many different fibers each of which is of unknown ultimate size.\n",
    "\n",
    "- It is not known when a fiber in the lower rank of __a_swapped__ is completed until the traversal of __a__ has finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_swapped = Tensor(rank_ids=[\"R0\", \"R1\"])\n",
    "a_swapped.setColor(\"blue\")\n",
    "a_swapped.setName(\"A_swapped\")\n",
    "\n",
    "a_r1 = a.getRoot()\n",
    "a_swapped_r0 = a_swapped.getRoot()\n",
    "\n",
    "canvas = createCanvas(a, a_swapped)\n",
    "\n",
    "for r1, a_r0 in a_r1:\n",
    "    for r0, a_val in a_r0:\n",
    "        a_swapped_r1 = a_swapped_r0.getPayloadRef(r0)\n",
    "        a_swapped_r1.append(r1, a_val)\n",
    "        \n",
    "        canvas.addFrame((r1, r0), (r0, r1))\n",
    "        \n",
    "displayTensor(a_swapped)\n",
    "displayCanvas(canvas)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Update batch sequence\n",
    "\n",
    "Do a (concordant) traversal of the rank-2 tensor (__a__), and for each element of the tensor log the coordinates (__r1__ and __r0__) and the value (__a_val__) as a tuple in a new rank-2 tensor (__bins__). When logging, select a bin id (top rank coordinate of __bins__) by a partitioning of the bottom rank (__r0__) coordinates of __a__ (e.g., divide the bottom rank coordinate of __a__ by 2). \n",
    "\n",
    "Observations:\n",
    "\n",
    "- Insert/updates to top rank of bins is discordant, but to a much smaller number of coordinates than in the one-pass implementation\n",
    "\n",
    "- Note that the additions to the fibers in the lower rank of __bins__ is always just at the end of the fibers, so each of those fibers can be streamed to a larger storage array (although the ultimate size of each fiber is not known statically).\n",
    "\n",
    "\n",
    "TBD: Maybe consider using the tuple (__r1__, __r0__) as the coordinate for the lower rank of __bins__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coordinates_per_bin = 2\n",
    "\n",
    "bins = Tensor(rank_ids=[\"B\", \"N\"])\n",
    "bins.setColor(\"purple\").setName(\"bins\")\n",
    "\n",
    "a_r1 = a.getRoot()\n",
    "bins_b = bins.getRoot()\n",
    "\n",
    "canvas = createCanvas(a, bins)\n",
    "n = 0\n",
    "\n",
    "for r1, a_r0 in a_r1:\n",
    "    for r0, a_val in a_r0:\n",
    "        n += 1\n",
    "        \n",
    "        b = r0//coordinates_per_bin\n",
    "        bins_n = bins_b.getPayloadRef(b)\n",
    "        \n",
    "        bins_n.append(n, (r0, r1, a_val))\n",
    "        canvas.addFrame([(r1, r0)], [(b, n)])\n",
    "        \n",
    "displayTensor(bins)\n",
    "displayCanvas(canvas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2  - Replay the log\n",
    "\n",
    "Do a concordant traversal of all the elements of the log tensor (__bins__) and create the swapped tensor (__a_swapped__) by adding fibers (and elements) while processing each bin (top rank coordinate of __bins__). Specifically, for each payload (__r0__, __r1__, and __a_val__) update (or insert/update) a fiber into the new tensor (__a_swapped__) at the swapped top rank coordinate (__r0__) and append to that fiber the proper value (__a_val__) at the proper lower rank coordinate (__r1__). \n",
    "\n",
    "Observations:\n",
    "\n",
    "- During each time interval, i.e., the time spent working on a bin, the range of coordinates for insert/updates of the upper rank of the swapped tensor (__a_swapped))__ is small, which allows the __getPayloadRef()__ to be efficient (and amenable to shortcuts - see cells below)\n",
    "\n",
    "- All updates to the coordinates in the new tensor (__a_swapped__) associated with a bin are done when the bin is done, so that part of the new tensor can be streamed out to another storage level immediately.\n",
    "\n",
    "- The operations on the fibers in the lower rank of the new tensor (__a_swapped__) are aways an append. However, it still could be challenging to directly create a compressed represenation, e.g., a payload/coordlist, for the multiple fibers created from a single bin, because they are created in parallel and the ultimate sizes of each fiber are unknown. A least they are more bounded in size, so can be created in a smaller storage unit and concatenated when streamed to the a larger storage unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_swapped = Tensor(rank_ids=[\"R0\", \"R1\"])\n",
    "a_swapped.setColor(\"blue\")\n",
    "a_swapped.setName(\"A_swapped\")\n",
    "\n",
    "bins_b = bins.getRoot()\n",
    "a_swapped_r0 = a_swapped.getRoot()\n",
    "\n",
    "canvas = createCanvas(bins, a_swapped)\n",
    "\n",
    "for b, bins_n in bins_b:\n",
    "    for n, (r0, r1, a_val) in bins_n:\n",
    "        a_swapped_r1 = a_swapped_r0.getPayloadRef(r0)\n",
    "        a_swapped_r1.append(r1, a_val)\n",
    "        canvas.addFrame((b, n), (r0, r1))\n",
    "        \n",
    "        \n",
    "displayTensor(a_swapped)\n",
    "displayCanvas(canvas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "a_swapped.getRoot() == a.swapRanks().getRoot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2  - Replay the log - with shortcuts\n",
    "\n",
    "Given the nice pattern of the values returned by the __getPayloadRef()__ method call exhibited by the above dataflow, one can optimize the search for the payload associated with the desired coordinate in the __getPayloadRef()__ call by using the \"start_pos\" shortcut. The following cell displays a control to enable or disable the use of the shortcut for the following log replay dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEnableControl(\"Use shortcut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a_swapped = Tensor(rank_ids=[\"R0\", \"R1\"])\n",
    "a_swapped.setColor(\"blue\")\n",
    "a_swapped.setName(\"A_swapped\")\n",
    "\n",
    "bins_b = bins.getRoot()\n",
    "a_swapped_r0 = a_swapped.getRoot()\n",
    "\n",
    "canvas = createCanvas(bins, a_swapped)\n",
    "\n",
    "next_start_pos = 0\n",
    "\n",
    "for b, bins_n in bins_b:\n",
    "    start_pos = next_start_pos\n",
    "    \n",
    "    for n, (r0, r1, a_val) in bins_n:\n",
    "        a_swapped_r1 = a_swapped_r0.getPayloadRef(r0, start_pos=start_pos)\n",
    "        if enable[\"Use shortcut\"]:\n",
    "            next_start_pos = max(next_start_pos, a_swapped_r0.getSavedPos())\n",
    "                \n",
    "        a_swapped_r1.append(r1, a_val)\n",
    "        canvas.addFrame((b, n), (r0, r1))\n",
    "        \n",
    "(n, distance) = a_swapped_r0.getSavedPosStats()\n",
    "print(f\"Average search distance = {distance/n:4.2f}\")\n",
    "      \n",
    "displayTensor(a_swapped)\n",
    "displayCanvas(canvas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing area\n",
    "\n",
    "For running alternative algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
